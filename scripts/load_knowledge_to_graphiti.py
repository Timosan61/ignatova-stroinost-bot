#!/usr/bin/env python3
"""
Graphiti Knowledge Loader

–ó–∞–≥—Ä—É–∑–∫–∞ parsed knowledge base entities –≤ Neo4j —á–µ—Ä–µ–∑ Graphiti.

Features:
- Batch loading —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º (tqdm)
- Checkpoints –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
- Error handling –∏ retry logic
- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (Tier 1 ‚Üí Tier 3)
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏

Usage:
    python scripts/load_knowledge_to_graphiti.py [--tier 1|2|3] [--batch-size 50]
"""

import os
import sys
import json
import asyncio
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging

try:
    from tqdm import tqdm
except ImportError:
    print("‚ö†Ô∏è tqdm not installed. Install: pip install tqdm")
    sys.exit(1)

try:
    from neo4j.exceptions import ServiceUnavailable, ClientError
except ImportError:
    # Fallback –µ—Å–ª–∏ neo4j –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏–∑–æ–π—Ç–∏)
    ServiceUnavailable = Exception
    ClientError = Exception

# Add root to PYTHONPATH
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from bot.services.graphiti_service import get_graphiti_service
from bot.services.graphiti_checkpoint_service import get_checkpoint_service
from bot.models.knowledge_entities import (
    FAQEntry,
    CourseLesson,
    CuratorCorrection,
    StudentQuestion,
    create_episode_metadata
)

logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class GraphitiLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤ Graphiti —Å MySQL checkpoint"""

    def __init__(self, parsed_dir: Path, tier: Optional[int] = None, batch_number: int = 0):
        self.parsed_dir = parsed_dir
        self.tier = tier
        self.batch_number = batch_number
        self.graphiti_service = get_graphiti_service()
        self.checkpoint_service = get_checkpoint_service()

        self.stats = {
            "total": 0,
            "success": 0,
            "failed": 0,
            "skipped": 0
        }

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å checkpoint statistics –∏–∑ MySQL
        checkpoint_stats = self.checkpoint_service.get_stats()
        logger.info(f"üìÇ MySQL Checkpoint loaded: {checkpoint_stats['total']} entities already loaded")
        if checkpoint_stats['by_type']:
            for entity_type, count in checkpoint_stats['by_type'].items():
                logger.info(f"   - {entity_type}: {count}")

    async def load_entity(self, entity: Any, entity_id: str, entity_type: str, max_retries: int = 10) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–∏–Ω entity –≤ Graphiti —Å –¥–≤–æ–π–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

        Args:
            entity: Pydantic entity
            entity_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            entity_type: –¢–∏–ø entity (–¥–ª—è checkpoint)
            max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (—É–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 10 –¥–ª—è Neo4j token refresh)

        Returns:
            True if success, False otherwise
        """
        # –î–í–û–ô–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í:
        # 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å MySQL checkpoint (–±—ã—Å—Ç—Ä–æ)
        if self.checkpoint_service.is_loaded(entity_id):
            self.stats["skipped"] += 1
            return True

        # 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Neo4j (—Ä–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ checkpoint –ø–æ—Ç–µ—Ä—è–Ω)
        entity_exists = await self.graphiti_service.entity_exists(entity_id)
        if entity_exists:
            # Entity –µ—Å—Ç—å –≤ Neo4j –Ω–æ –Ω–µ—Ç –≤ checkpoint - –¥–æ–±–∞–≤–∏–º –≤ checkpoint
            self.checkpoint_service.mark_loaded(
                entity_id=entity_id,
                entity_type=entity_type,
                tier=self.tier,
                batch_number=self.batch_number
            )
            self.stats["skipped"] += 1
            logger.debug(f"Entity {entity_id} found in Neo4j but not in checkpoint - updated checkpoint")
            return True

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ Episode content
        content = entity.to_episode_content()
        metadata = create_episode_metadata(entity)

        # –ü–æ–ø—ã—Ç–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Å retry
        for attempt in range(max_retries):
            try:
                success, result = await self.graphiti_service.add_episode(
                    content=content,
                    episode_type=entity.entity_type.value,
                    metadata=metadata,
                    source_description=f"{entity.entity_type.value} from knowledge base"
                )

                if success:
                    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ MySQL checkpoint
                    self.checkpoint_service.mark_loaded(
                        entity_id=entity_id,
                        entity_type=entity_type,
                        episode_id=result,
                        tier=self.tier,
                        batch_number=self.batch_number
                    )
                    self.stats["success"] += 1
                    return True
                else:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(1 * (attempt + 1))  # Exponential backoff
                    else:
                        logger.error(f"‚ùå Failed to load {entity_id}: {result}")
                        self.stats["failed"] += 1
                        return False

            except ServiceUnavailable as e:
                # Neo4j —Ç–æ–∫–µ–Ω –∏—Å—Ç—ë–∫ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞)
                if attempt < max_retries - 1:
                    backoff_seconds = min(10 + (attempt * 2), 30)  # 10s, 12s, 14s ... –¥–æ 30s
                    logger.warning(f"‚ö†Ô∏è Neo4j unavailable (token refresh?), retry {attempt + 1}/{max_retries} for {entity_id} after {backoff_seconds}s")
                    await asyncio.sleep(backoff_seconds)
                else:
                    logger.error(f"‚ùå Neo4j unavailable for {entity_id} after {max_retries} attempts: {e}")
                    self.stats["failed"] += 1
                    return False

            except ClientError as e:
                # Neo4j query error - –Ω–µ retry, —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
                logger.error(f"‚ùå Neo4j client error for {entity_id}: {e}")
                self.stats["failed"] += 1
                return False

            except Exception as e:
                # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ - retry —Å exponential backoff
                if attempt < max_retries - 1:
                    backoff_seconds = 2 * (attempt + 1)  # 2s, 4s, 6s, 8s ...
                    logger.warning(f"‚ö†Ô∏è Retry {attempt + 1}/{max_retries} for {entity_id} after {backoff_seconds}s: {type(e).__name__}: {e}")
                    await asyncio.sleep(backoff_seconds)
                else:
                    logger.error(f"‚ùå Failed {entity_id} after {max_retries} attempts: {type(e).__name__}: {e}")
                    self.stats["failed"] += 1
                    return False

        return False

    async def load_batch(self, entities: List[Any], entity_type: str, batch_size: int = 50):
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å batch of entities

        Args:
            entities: List of parsed entities
            entity_type: Type name (–¥–ª—è –ª–æ–≥–æ–≤ –∏ checkpoint)
            batch_size: –†–∞–∑–º–µ—Ä batch
        """
        logger.info(f"\nüì¶ Loading {len(entities)} {entity_type} entities...")

        # Progress bar
        pbar = tqdm(total=len(entities), desc=f"Loading {entity_type}")

        for i in range(0, len(entities), batch_size):
            batch = entities[i:i + batch_size]
            batch_num = i // batch_size
            self.batch_number = batch_num

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ batch
            tasks = []
            for idx, entity in enumerate(batch):
                entity_id = f"{entity_type}_{i + idx}"
                # –ü–µ—Ä–µ–¥–∞—ë–º entity_type –¥–ª—è checkpoint
                tasks.append(self.load_entity(entity, entity_id, entity_type))

            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö tasks –≤ batch
            await asyncio.gather(*tasks)

            # Update progress
            pbar.update(len(batch))

            # MySQL checkpoint —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ load_entity()
            logger.debug(f"Batch {batch_num} completed ({i + len(batch)}/{len(entities)})")

        pbar.close()

        logger.info(f"‚úÖ Finished loading {entity_type}: {self.stats['success']} success, {self.stats['skipped']} skipped, {self.stats['failed']} failed")

    async def load_tier(self, tier: int, batch_size: int = 50):
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π tier –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

        Tier 1: FAQ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        Tier 2: Lessons + Corrections
        Tier 3: Questions + Brainwrites (TODO)

        Args:
            tier: –ù–æ–º–µ—Ä tier (1-3)
            batch_size: –†–∞–∑–º–µ—Ä batch
        """
        if tier == 1:
            # Tier 1: FAQ - —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –∏ —á–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã
            logger.info("\nüéØ TIER 1: Loading FAQ (TOP priority)")

            faq_file = self.parsed_dir / "parsed_faq.json"
            if faq_file.exists():
                with open(faq_file, 'r', encoding='utf-8') as f:
                    faq_data = json.load(f)

                entities = [FAQEntry(**item) for item in faq_data]
                self.stats["total"] += len(entities)
                await self.load_batch(entities, "FAQ", batch_size)
            else:
                logger.warning(f"‚ö†Ô∏è FAQ file not found: {faq_file}")

        elif tier == 2:
            # Tier 2: Lessons + Corrections
            logger.info("\nüìö TIER 2: Loading Lessons + Corrections")

            # Lessons
            lessons_file = self.parsed_dir / "parsed_lessons.json"
            if lessons_file.exists():
                with open(lessons_file, 'r', encoding='utf-8') as f:
                    lessons_data = json.load(f)

                entities = [CourseLesson(**item) for item in lessons_data]
                self.stats["total"] += len(entities)
                await self.load_batch(entities, "Lesson", batch_size)
            else:
                logger.warning(f"‚ö†Ô∏è Lessons file not found: {lessons_file}")

            # Corrections
            corrections_file = self.parsed_dir / "parsed_corrections.json"
            if corrections_file.exists():
                with open(corrections_file, 'r', encoding='utf-8') as f:
                    corrections_data = json.load(f)

                entities = [CuratorCorrection(**item) for item in corrections_data]
                self.stats["total"] += len(entities)
                await self.load_batch(entities, "Correction", batch_size)
            else:
                logger.warning(f"‚ö†Ô∏è Corrections file not found: {corrections_file}")

        elif tier == 3:
            # Tier 3: Questions only (Brainwrites excluded)
            logger.info("\nüí¨ TIER 3: Loading Questions")

            # Student Questions
            questions_file = self.parsed_dir / "parsed_questions.json"
            if questions_file.exists():
                with open(questions_file, 'r', encoding='utf-8') as f:
                    questions_data = json.load(f)

                entities = [StudentQuestion(**item) for item in questions_data]
                self.stats["total"] += len(entities)
                await self.load_batch(entities, "Question", batch_size)
            else:
                logger.warning(f"‚ö†Ô∏è Questions file not found: {questions_file}")

            # Note: Brainwrite Examples excluded - student examples may not follow exact methodology

        else:
            logger.error(f"‚ùå Invalid tier: {tier}. Must be 1, 2, or 3")

    async def load_all(self, batch_size: int = 50):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π (–≤—Å–µ tiers)"""
        logger.info("üöÄ Loading ALL knowledge base tiers...")

        for tier in [1, 2, 3]:  # –í—Å–µ 3 —Ç–∏—Ä–∞
            await self.load_tier(tier, batch_size)

        logger.info("\n‚úÖ All tiers loaded!")

    def print_stats(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏"""
        logger.info("\n" + "=" * 60)
        logger.info("üìä LOADING STATISTICS")
        logger.info("=" * 60)
        logger.info(f"Total entities: {self.stats['total']}")
        logger.info(f"Successfully loaded: {self.stats['success']} ({self.stats['success'] / max(1, self.stats['total']) * 100:.1f}%)")
        logger.info(f"Failed: {self.stats['failed']}")
        logger.info(f"Skipped (already loaded): {self.stats['skipped']}")
        logger.info("=" * 60)


async def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Load knowledge base to Graphiti")
    parser.add_argument("--tier", type=int, choices=[1, 2, 3], help="Load specific tier (1=FAQ, 2=Lessons+Corrections, 3=Questions+Brainwrites)")
    parser.add_argument("--all", action="store_true", help="Load all tiers")
    parser.add_argument("--batch-size", type=int, default=50, help="Batch size (default: 50)")
    parser.add_argument("--reset-checkpoint", action="store_true", help="Reset MySQL checkpoint (start from scratch)")

    args = parser.parse_args()

    # Paths
    parsed_dir = root_dir / "data" / "parsed_kb"

    # Check parsed data exists
    if not parsed_dir.exists():
        logger.error(f"‚ùå Parsed data not found: {parsed_dir}")
        logger.info("\nüí° Run parser first: python scripts/parse_knowledge_base.py")
        return

    # Reset checkpoint if requested
    if args.reset_checkpoint:
        checkpoint_service = get_checkpoint_service()
        if checkpoint_service.clear_all():
            logger.info("üóëÔ∏è MySQL checkpoint reset - all entries cleared")
        else:
            logger.warning("‚ö†Ô∏è Failed to reset checkpoint (database might be disabled)")

    # Initialize loader with tier (will be updated for each tier)
    loader = GraphitiLoader(parsed_dir, tier=args.tier)

    # Check Graphiti service
    if not loader.graphiti_service.enabled:
        logger.error("‚ùå Graphiti service not enabled!")
        logger.info("\nüí° Setup Neo4j first:")
        logger.info("   1. Create Neo4j Aura instance (https://neo4j.com/cloud/aura/)")
        logger.info("   2. Add to .env:")
        logger.info("      NEO4J_URI=neo4j+s://xxxxx.databases.neo4j.io")
        logger.info("      NEO4J_PASSWORD=your_password")
        logger.info("      GRAPHITI_ENABLED=true")
        logger.info("   3. Test connection: python scripts/test_neo4j_connection.py")
        return

    # Load knowledge
    if args.tier:
        await loader.load_tier(args.tier, args.batch_size)
    elif args.all:
        await loader.load_all(args.batch_size)
    else:
        # Default: load Tier 1 (FAQ) only
        logger.info("üí° No tier specified, loading Tier 1 (FAQ) by default")
        logger.info("   Use --all to load all tiers, or --tier N to load specific tier")
        await loader.load_tier(1, args.batch_size)

    # Print stats
    loader.print_stats()

    # Final checkpoint
    loader.save_checkpoint()
    logger.info(f"\nüíæ Checkpoint saved: {checkpoint_file}")


if __name__ == "__main__":
    asyncio.run(main())
