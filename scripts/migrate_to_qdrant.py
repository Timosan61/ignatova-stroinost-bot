#!/usr/bin/env python3
"""
Migrate Knowledge Base to Qdrant

–°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –≤—Å–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∫—É—Ä—Å–∞ "–í—Å–µ–ø—Ä–æ—â–∞—é—â–∞—è" –≤ Qdrant Vector Database.

–ü—Ä–æ—Ü–µ—Å—Å:
1. –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (FAQ, —É—Ä–æ–∫–∏, —Ç–µ—Ö–Ω–∏–∫–∏, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏)
2. –°–æ–∑–¥–∞–Ω–∏–µ Qdrant collection (–µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings —á–µ—Ä–µ–∑ sentence-transformers
4. Batch upload entities –≤ Qdrant
5. Checkpoint system –¥–ª—è resumable loading

Usage:
    python3 scripts/migrate_to_qdrant.py --batch-size 50 --reset
"""

import os
import sys
import json
import asyncio
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –≤ PYTHONPATH
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from dotenv import load_dotenv
load_dotenv()

try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import (
        Distance, VectorParams, PointStruct,
        CreateCollection
    )
    from sentence_transformers import SentenceTransformer
    QDRANT_AVAILABLE = True
except ImportError:
    print("‚ùå ERROR: qdrant-client or sentence-transformers not installed")
    print("Install: pip install qdrant-client sentence-transformers")
    sys.exit(1)

from bot.config import QDRANT_URL, QDRANT_API_KEY, QDRANT_COLLECTION, EMBEDDING_MODEL
from bot.models.knowledge_entities import (
    CourseLesson, FAQEntry, BrainwriteTechnique,
    StudentQuestion, CuratorCorrection, BrainwriteExample
)

# Import parser
from parse_knowledge_base import KnowledgeBaseParser

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class QdrantMigration:
    """–ú–∏–≥—Ä–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤ Qdrant"""

    def __init__(
        self,
        kb_dir: Path,
        batch_size: int = 50,
        checkpoint_file: Optional[Path] = None
    ):
        """
        Args:
            kb_dir: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ KNOWLEDGE_BASE
            batch_size: –†–∞–∑–º–µ—Ä batch –¥–ª—è upload
            checkpoint_file: –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        self.kb_dir = kb_dir
        self.batch_size = batch_size
        self.checkpoint_file = checkpoint_file or (kb_dir / "qdrant_migration_checkpoint.json")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant client
        if not QDRANT_URL or not QDRANT_API_KEY:
            raise ValueError("QDRANT_URL and QDRANT_API_KEY must be configured")

        self.client = QdrantClient(
            url=QDRANT_URL,
            api_key=QDRANT_API_KEY,
            timeout=60
        )
        logger.info(f"‚úÖ Qdrant client connected: {QDRANT_URL}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è sentence transformer
        logger.info(f"Loading sentence transformer: {EMBEDDING_MODEL}")
        self.encoder = SentenceTransformer(EMBEDDING_MODEL)
        logger.info(f"‚úÖ Sentence transformer loaded: {EMBEDDING_MODEL}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_entities": 0,
            "uploaded_entities": 0,
            "failed_entities": 0,
            "by_type": {}
        }

    def _load_checkpoint(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å checkpoint (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)"""
        if self.checkpoint_file.exists():
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint = json.load(f)
                logger.info(f"üì• Checkpoint loaded: {checkpoint['uploaded_entities']} entities already uploaded")
                return checkpoint
        return {"uploaded_ids": [], "uploaded_entities": 0}

    def _save_checkpoint(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å checkpoint"""
        checkpoint = {
            "uploaded_ids": list(self.uploaded_ids),
            "uploaded_entities": self.stats["uploaded_entities"],
            "timestamp": datetime.utcnow().isoformat()
        }
        with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint, f, indent=2, ensure_ascii=False)
        logger.info(f"üíæ Checkpoint saved: {self.stats['uploaded_entities']} entities")

    def _ensure_collection(self):
        """–°–æ–∑–¥–∞—Ç—å collection –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
        collections = self.client.get_collections()
        collection_names = [c.name for c in collections.collections]

        if QDRANT_COLLECTION in collection_names:
            logger.info(f"‚úÖ Collection '{QDRANT_COLLECTION}' already exists")
            return

        logger.info(f"Creating collection: {QDRANT_COLLECTION}")

        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –∏–∑ –º–æ–¥–µ–ª–∏
        test_vector = self.encoder.encode("test").tolist()
        vector_size = len(test_vector)

        self.client.create_collection(
            collection_name=QDRANT_COLLECTION,
            vectors_config=VectorParams(
                size=vector_size,
                distance=Distance.COSINE
            )
        )
        logger.info(f"‚úÖ Collection '{QDRANT_COLLECTION}' created (vector_size={vector_size})")

    def _parse_all_entities(self) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö entities –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

        Returns:
            List of entities –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
            [
                {
                    "id": "lesson_1_chunk_0",
                    "entity_type": "lesson",
                    "title": "–£—Ä–æ–∫ 1: –í–≤–µ–¥–µ–Ω–∏–µ",
                    "content": "...",
                    "metadata": {...}
                },
                ...
            ]
        """
        parser = KnowledgeBaseParser(self.kb_dir)

        all_entities = []
        entity_id = 0

        # 1. Parse FAQ
        faq_file = self.kb_dir / "FAQ_EXTENDED.md"
        if faq_file.exists():
            logger.info("üìñ Parsing FAQ...")
            faq_entries = parser.parse_faq(faq_file)
            for faq in faq_entries:
                entity = {
                    "id": f"faq_{entity_id}",
                    "entity_type": "faq",
                    "title": faq.question[:100],  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –≤–æ–ø—Ä–æ—Å–∞
                    "content": faq.to_episode_content(),
                    "metadata": {
                        "category": faq.category,
                        "keywords": faq.keywords,
                        "frequency": faq.frequency
                    }
                }
                all_entities.append(entity)
                entity_id += 1

            logger.info(f"‚úÖ FAQ parsed: {len(faq_entries)} entries")

        # 2. Parse Lessons
        kb_file = self.kb_dir / "KNOWLEDGE_BASE_FULL.md"
        if kb_file.exists():
            logger.info("üìñ Parsing Lessons...")
            lessons = parser.parse_lessons(kb_file)
            for lesson in lessons:
                entity = {
                    "id": f"lesson_{lesson.lesson_number}_chunk_{lesson.chunk_index or 0}",
                    "entity_type": "lesson",
                    "title": lesson.title,
                    "content": lesson.to_episode_content(),
                    "metadata": {
                        "lesson_number": lesson.lesson_number,
                        "category": lesson.category.value,
                        "chunk_index": lesson.chunk_index,
                        "total_chunks": lesson.total_chunks,
                        "key_concepts": lesson.key_concepts
                    }
                }
                all_entities.append(entity)
                entity_id += 1

            logger.info(f"‚úÖ Lessons parsed: {len(lessons)} chunks")

        # 3. Parse Curator Corrections
        corrections_file = self.kb_dir / "curator_corrections_ALL.json"
        if corrections_file.exists():
            logger.info("üìñ Parsing Curator Corrections...")
            corrections = parser.parse_corrections(corrections_file)
            for correction in corrections:
                entity = {
                    "id": f"correction_{entity_id}",
                    "entity_type": "correction",
                    "title": correction.student_text[:100] if correction.student_text else correction.error_type,
                    "content": correction.to_episode_content(),
                    "metadata": {
                        "error_type": correction.error_type,
                        "related_technique": correction.related_technique,
                        "related_lesson": correction.related_lesson,
                        "curator_name": correction.curator_name,
                        "student_name": correction.student_name,
                        "has_explanation": bool(correction.explanation)
                    }
                }
                all_entities.append(entity)
                entity_id += 1

            logger.info(f"‚úÖ Curator Corrections parsed: {len(corrections)} entries")

        logger.info(f"üìä Total entities parsed: {len(all_entities)}")
        return all_entities

    def _upload_batch(self, entities: List[Dict[str, Any]]):
        """
        Upload batch of entities to Qdrant

        Args:
            entities: List of entities to upload
        """
        points = []

        for entity in entities:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –¥–ª—è content
            vector = self.encoder.encode(entity["content"]).tolist()

            # –°–æ–∑–¥–∞—ë–º payload
            payload = {
                "entity_type": entity["entity_type"],
                "title": entity["title"],
                "content": entity["content"],
                "metadata": entity["metadata"],
                "created_at": datetime.utcnow().isoformat()
            }

            # –°–æ–∑–¥–∞—ë–º point
            point = PointStruct(
                id=entity["id"],
                vector=vector,
                payload=payload
            )
            points.append(point)

        # Upload batch
        self.client.upsert(
            collection_name=QDRANT_COLLECTION,
            points=points
        )

        self.stats["uploaded_entities"] += len(points)

        # –û–±–Ω–æ–≤–ª—è–µ–º uploaded_ids
        for entity in entities:
            self.uploaded_ids.add(entity["id"])

    async def migrate(self, reset: bool = False):
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é

        Args:
            reset: –ï—Å–ª–∏ True - —É–¥–∞–ª–∏—Ç—å checkpoint –∏ –Ω–∞—á–∞—Ç—å —Å –Ω—É–ª—è
        """
        logger.info("üöÄ Starting Qdrant migration...")

        # –£–¥–∞–ª–∏—Ç—å checkpoint –µ—Å–ª–∏ reset
        if reset and self.checkpoint_file.exists():
            self.checkpoint_file.unlink()
            logger.info("üóëÔ∏è Checkpoint deleted (reset=True)")

        # –°–æ–∑–¥–∞—Ç—å collection
        self._ensure_collection()

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å checkpoint
        checkpoint = self._load_checkpoint()
        self.uploaded_ids = set(checkpoint.get("uploaded_ids", []))
        self.stats["uploaded_entities"] = checkpoint.get("uploaded_entities", 0)

        # –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö entities
        logger.info("üìñ Parsing knowledge base...")
        all_entities = self._parse_all_entities()
        self.stats["total_entities"] = len(all_entities)

        # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ entities
        entities_to_upload = [e for e in all_entities if e["id"] not in self.uploaded_ids]

        if not entities_to_upload:
            logger.info("‚úÖ All entities already uploaded!")
            self._print_stats()
            return

        logger.info(f"üì§ Uploading {len(entities_to_upload)} entities (batch_size={self.batch_size})...")

        # Upload batches
        for i in range(0, len(entities_to_upload), self.batch_size):
            batch = entities_to_upload[i:i + self.batch_size]

            try:
                self._upload_batch(batch)

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º
                for entity in batch:
                    entity_type = entity["entity_type"]
                    self.stats["by_type"][entity_type] = self.stats["by_type"].get(entity_type, 0) + 1

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ batch
                self._save_checkpoint()

                logger.info(
                    f"‚úÖ Batch {i // self.batch_size + 1}/{(len(entities_to_upload) - 1) // self.batch_size + 1} "
                    f"uploaded ({self.stats['uploaded_entities']}/{self.stats['total_entities']})"
                )

            except Exception as e:
                logger.error(f"‚ùå Failed to upload batch: {e}")
                logger.exception("Full traceback:")
                self.stats["failed_entities"] += len(batch)

        logger.info("‚úÖ Migration completed!")
        self._print_stats()

    def _print_stats(self):
        """–í—ã–≤–µ—Å—Ç–∏ —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        logger.info("=" * 60)
        logger.info("üìä MIGRATION STATISTICS")
        logger.info("=" * 60)
        logger.info(f"Total entities:    {self.stats['total_entities']}")
        logger.info(f"Uploaded:          {self.stats['uploaded_entities']}")
        logger.info(f"Failed:            {self.stats['failed_entities']}")
        logger.info("")
        logger.info("By entity type:")
        for entity_type, count in self.stats["by_type"].items():
            logger.info(f"  - {entity_type:15s} {count:4d}")
        logger.info("=" * 60)


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="Migrate knowledge base to Qdrant")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="Batch size for upload (default: 50)"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Reset checkpoint and start from scratch"
    )
    parser.add_argument(
        "--kb-dir",
        type=Path,
        default=root_dir / "KNOWLEDGE_BASE",
        help="Path to KNOWLEDGE_BASE directory"
    )

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ kb_dir —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if not args.kb_dir.exists():
        logger.error(f"‚ùå KNOWLEDGE_BASE directory not found: {args.kb_dir}")
        sys.exit(1)

    # –ó–∞–ø—É—Å–∫ –º–∏–≥—Ä–∞—Ü–∏–∏
    migration = QdrantMigration(
        kb_dir=args.kb_dir,
        batch_size=args.batch_size
    )

    asyncio.run(migration.migrate(reset=args.reset))


if __name__ == "__main__":
    main()
