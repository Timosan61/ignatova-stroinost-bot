#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø—Ä–æ–¥–∞–∂ –≤ Zep Knowledge Graph.
–†–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Ö –≤ Zep.
"""

import os
import re
import json
import asyncio
import logging
from typing import List, Dict, Any
from datetime import datetime
from dataclasses import dataclass

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
from zep_cloud.client import AsyncZep

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class KnowledgeChunk:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —á–∞–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π"""
    content: str
    title: str
    category: str
    source: str
    metadata: Dict[str, Any]

class KnowledgeBaseLoader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤ Zep"""
    
    def __init__(self, zep_api_key: str):
        self.zep_client = AsyncZep(api_key=zep_api_key)
        self.project_id = "sales-knowledge-base"
        
    def split_markdown_by_sections(self, content: str) -> List[KnowledgeChunk]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç markdown —Ñ–∞–π–ª –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ –ø–æ —Å–µ–∫—Ü–∏—è–º"""
        chunks = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–µ–∫—Ü–∏–π
        patterns = {
            'training': r'## –¢—Ä–µ–Ω–∏–Ω–≥.*?call_(\d+)_(summary|FAQ)\.md',
            'section': r'## (.+?)(?=\n\n|\n##|\nfollowed by another section|\Z)',
            'call_section': r'# (.+?)(?=\n##|\n#|\Z)',
            'faq': r'(\d+)\) \*\*(.+?)\*\* ‚Äî (.+?)(?=\n\d+\)|\n##|\n#|\Z)',
            'script': r'### (.+?)(?=\n###|\n##|\n#|\Z)'
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –≥–ª–∞–≤–Ω—ã–º —Å–µ–∫—Ü–∏—è–º (## –∑–∞–≥–æ–ª–æ–≤–∫–∏)
        main_sections = re.split(r'\n(?=## )', content)
        
        for section in main_sections:
            if not section.strip():
                continue
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–µ–∫—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            chunk = self._process_section(section)
            if chunk:
                chunks.append(chunk)
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
        return chunks
    
    def _process_section(self, section: str) -> KnowledgeChunk:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å–µ–∫—Ü–∏—é –∏ —Å–æ–∑–¥–∞—ë—Ç —á–∞–Ω–∫"""
        lines = section.strip().split('\n')
        
        if not lines:
            return None
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞)
        title = lines[0].strip('# ').strip()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        category = self._determine_category(section)
        source = self._extract_source(section)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 1000 —Ç–æ–∫–µ–Ω–æ–≤ = 4000 —Å–∏–º–≤–æ–ª–æ–≤)
        max_chars = 3500
        content = section
        if len(content) > max_chars:
            # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–º—ã—Å–ª
            sentences = re.split(r'(?<=[.!?])\s+', content)
            truncated = ""
            for sentence in sentences:
                if len(truncated + sentence) > max_chars:
                    break
                truncated += sentence + " "
            content = truncated.strip()
        
        # –°–æ–∑–¥–∞—ë–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'word_count': len(content.split()),
            'char_count': len(content),
            'created_at': datetime.now().isoformat(),
            'contains_scripts': '—Å–∫—Ä–∏–ø—Ç' in content.lower() or '—à–∞–±–ª–æ–Ω' in content.lower(),
            'contains_objections': '–≤–æ–∑—Ä–∞–∂–µ–Ω–∏' in content.lower(),
            'contains_faq': 'FAQ' in section or '–≤–æ–ø—Ä–æ—Å' in content.lower(),
        }
        
        return KnowledgeChunk(
            content=content,
            title=title,
            category=category,
            source=source,
            metadata=metadata
        )
    
    def _determine_category(self, section: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —á–∞–Ω–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        section_lower = section.lower()
        
        if 'call_' in section_lower and 'summary' in section_lower:
            return 'training_summary'
        elif 'call_' in section_lower and 'faq' in section_lower:
            return 'training_faq'
        elif '—Å–∫—Ä–∏–ø—Ç' in section_lower or '—à–∞–±–ª–æ–Ω' in section_lower:
            return 'scripts'
        elif '–≤–æ–∑—Ä–∞–∂–µ–Ω–∏' in section_lower:
            return 'objections'
        elif 'faq' in section_lower or '–≤–æ–ø—Ä–æ—Å' in section_lower:
            return 'faq'
        elif '—Ç–µ—Ö–Ω–∏–∫' in section_lower or '–º–µ—Ç–æ–¥' in section_lower:
            return 'techniques'
        elif '–ø—Ä–æ–¥–∞–∂' in section_lower or '–ø—Ä–æ–¥–∞–≤' in section_lower:
            return 'sales_methodology'
        else:
            return 'general'
    
    def _extract_source(self, section: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ (–Ω–æ–º–µ—Ä call'–∞) –∏–∑ —Å–µ–∫—Ü–∏–∏"""
        match = re.search(r'call_(\d+)', section)
        if match:
            return f"call_{match.group(1)}"
        return "general"
    
    async def upload_to_zep(self, chunks: List[KnowledgeChunk]) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞–Ω–∫–∏ –≤ Zep Knowledge Graph"""
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ Zep...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç
            for i, chunk in enumerate(chunks):
                try:
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Zep
                    document_data = {
                        "content": chunk.content,
                        "metadata": {
                            "title": chunk.title,
                            "category": chunk.category,
                            "source": chunk.source,
                            "chunk_id": i + 1,
                            **chunk.metadata
                        }
                    }
                    
                    # –°–æ–∑–¥–∞—ë–º —Å–µ—Å—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                    session_id = f"knowledge_{chunk.category}_{chunk.source}_{i}"
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
                    # –í Zep v2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è add_data –º–µ—Ç–æ–¥ –¥–ª—è Knowledge Graph
                    await self.zep_client.graph.add(
                        session_id=session_id,
                        data=json.dumps(document_data)
                    )
                    
                    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —á–∞–Ω–∫ {i+1}/{len(chunks)}: {chunk.title[:50]}...")
                    
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–∞ {i+1}: {e}")
                    continue
            
            logger.info("üéâ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤ Zep –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Zep: {e}")
            return False
    
    async def test_search(self, query: str) -> List[Dict]:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫ –ø–æ –≥—Ä–∞—Ñ—É –∑–Ω–∞–Ω–∏–π
            results = await self.zep_client.graph.search(
                query=query,
                limit=5
            )
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
    zep_api_key = os.getenv('ZEP_API_KEY')
    if not zep_api_key:
        logger.error("‚ùå ZEP_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è!")
        return
    
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
    knowledge_file = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 
        'Sales_Assistant_Master_Instruction_CONTENT_ONLY.md'
    )
    
    if not os.path.exists(knowledge_file):
        logger.error(f"‚ùå –§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {knowledge_file}")
        return
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    logger.info(f"üìñ –ß–∏—Ç–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ {knowledge_file}")
    with open(knowledge_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    logger.info(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –°–æ–∑–¥–∞—ë–º –∑–∞–≥—Ä—É–∑—á–∏–∫
    loader = KnowledgeBaseLoader(zep_api_key)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    chunks = loader.split_markdown_by_sections(content)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    logger.info(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞–Ω–∫–æ–≤:")
    categories = {}
    for chunk in chunks:
        categories[chunk.category] = categories.get(chunk.category, 0) + 1
    
    for category, count in categories.items():
        logger.info(f"  {category}: {count} —á–∞–Ω–∫–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Zep
    success = await loader.upload_to_zep(chunks)
    
    if success:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
        logger.info("\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π...")
        
        test_queries = [
            "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è–º–∏",
            "—Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è —Å—Ç–∞—Ä–æ–π –±–∞–∑—ã",
            "–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Å–∏—Ö–æ—Ç–∏–ø–∞",
            "–º–∞—Ä–∞—Ñ–æ–Ω—ã –ø–æ—Ö—É–¥–µ–Ω–∏—è —Ü–µ–Ω–∞"
        ]
        
        for query in test_queries:
            results = await loader.test_search(query)
            logger.info(f"  '{query}': {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

if __name__ == "__main__":
    asyncio.run(main())