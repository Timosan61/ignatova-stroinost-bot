#!/usr/bin/env python3
"""
Knowledge Base Parser

–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∫—É—Ä—Å–∞ "–í—Å–µ–ø—Ä–æ—â–∞—é—â–∞—è" –≤ structured entities.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
- Markdown (.md) - —É—Ä–æ–∫–∏, FAQ, –ø—Ä–∏–º–µ—Ä—ã
- JSON (.json) - –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏, –≤–æ–ø—Ä–æ—Å—ã, –º–æ–∑–≥–æ—Ä–∏—Ç–º—ã

Output: List of Pydantic entities –≥–æ—Ç–æ–≤—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Graphiti
"""

import re
import json
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import logging

# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –≤ PYTHONPATH
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from bot.models.knowledge_entities import (
    CourseLesson,
    LessonCategory,
    BrainwriteTechnique,
    StudentQuestion,
    CuratorCorrection,
    BrainwriteExample,
    FAQEntry,
    GlossaryEntry
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class KnowledgeBaseParser:
    """–ü–∞—Ä—Å–µ—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""

    def __init__(self, kb_dir: Path):
        """
        Args:
            kb_dir: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ KNOWLEDGE_BASE
        """
        self.kb_dir = kb_dir
        self.parsed_entities = []

    # ==================== FAQ PARSER ====================

    def parse_faq(self, file_path: Path) -> List[FAQEntry]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ FAQ_EXTENDED.md

        Returns:
            List of FAQEntry entities
        """
        logger.info(f"Parsing FAQ from {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        faq_entries = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤: **–í1.1.1: –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞?**
        question_pattern = r'\*\*–í([\d.]+):\s*(.+?)\?\*\*'

        # –ù–∞–π—Ç–∏ –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã
        questions = list(re.finditer(question_pattern, content))

        for i, q_match in enumerate(questions):
            question_id = f"FAQ_{q_match.group(1)}"
            question_text = q_match.group(2).strip() + "?"

            # –ù–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç (–º–µ–∂–¥—É —Ç–µ–∫—É—â–∏–º –≤–æ–ø—Ä–æ—Å–æ–º –∏ —Å–ª–µ–¥—É—é—â–∏–º)
            answer_start = q_match.end()
            answer_end = questions[i + 1].start() if i + 1 < len(questions) else len(content)

            answer_section = content[answer_start:answer_end]

            # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ (–ø–æ—Å–ª–µ **–û—Ç–≤–µ—Ç:**)
            answer_match = re.search(r'\*\*–û—Ç–≤–µ—Ç:\*\*\s*(.+)', answer_section, re.DOTALL)
            if not answer_match:
                logger.warning(f"No answer found for question {question_id}")
                continue

            answer_text = answer_match.group(1).strip()

            # –û—á–∏—Å—Ç–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
            next_q_in_answer = re.search(r'\*\*–í\d', answer_text)
            if next_q_in_answer:
                answer_text = answer_text[:next_q_in_answer.start()].strip()

            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Ä–∞–∑–¥–µ–ª—É
            category = self._extract_faq_category(content, q_match.start())

            # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_keywords(question_text + " " + answer_text)

            faq_entry = FAQEntry(
                faq_id=question_id,
                question=question_text,
                answer=answer_text[:5000],  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è safety
                category=category,
                frequency=None,  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∂–µ
                keywords=keywords,
                source_file=str(file_path.name)
            )

            faq_entries.append(faq_entry)

        logger.info(f"Parsed {len(faq_entries)} FAQ entries")
        return faq_entries

    def _extract_faq_category(self, content: str, position: int) -> str:
        """–ò–∑–≤–ª–µ—á—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é FAQ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–∞"""
        # –ù–∞–π—Ç–∏ –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞ –ø–µ—Ä–µ–¥ –≤–æ–ø—Ä–æ—Å–æ–º
        section_headers = list(re.finditer(r'### üìö –†–ê–ó–î–ï–õ \d+: (.+)', content))

        for header in reversed(section_headers):
            if header.start() < position:
                return header.group(1).strip()

        return "–û–±—â–µ–µ"

    def _extract_keywords(self, text: str, max_keywords: int = 10) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)"""
        # –£–¥–∞–ª–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        clean_text = re.sub(r'[^\w\s]', ' ', text.lower())

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        stop_words = {
            '—ç—Ç–æ', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—ã—Ç—å', '–¥–ª—è', '–∫–∞–∫',
            '—á—Ç–æ', '—á—Ç–æ–±—ã', '–∏–ª–∏', '–µ—Å–ª–∏', '–∫–æ–≥–¥–∞', '–≥–¥–µ', '—á–µ—Ä–µ–∑', '–ø–æ–¥'
        }

        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ (–¥–ª–∏–Ω–∞ > 4 —Å–∏–º–≤–æ–ª–æ–≤)
        words = [w for w in clean_text.split() if len(w) > 4 and w not in stop_words]
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1

        # –¢–æ–ø N –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:max_keywords]]

    # ==================== LESSONS PARSER ====================

    def parse_lessons(self, file_path: Path, chunk_size: int = 1000) -> List[CourseLesson]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ KNOWLEDGE_BASE_FULL.md

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —É—Ä–æ–∫–∞–º–∏
            chunk_size: –†–∞–∑–º–µ—Ä chunk –≤ —Å–ª–æ–≤–∞—Ö (–¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –±–æ–ª—å—à–∏—Ö —É—Ä–æ–∫–æ–≤)

        Returns:
            List of CourseLesson entities
        """
        logger.info(f"Parsing lessons from {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lessons = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —É—Ä–æ–∫–æ–≤: # –£–†–û–ö N: –ù–ê–ó–í–ê–ù–ò–ï
        lesson_pattern = r'<a id="—É—Ä–æ–∫-(\d+)"></a>\s*\n+# –£–†–û–ö (\d+): (.+?)\n'

        lesson_matches = list(re.finditer(lesson_pattern, content, re.IGNORECASE))

        for i, match in enumerate(lesson_matches):
            lesson_num = int(match.group(1))
            title = match.group(3).strip()

            # –ò–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —É—Ä–æ–∫–∞ (–¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —É—Ä–æ–∫–∞)
            content_start = match.end()
            content_end = lesson_matches[i + 1].start() if i + 1 < len(lesson_matches) else len(content)

            lesson_content = content[content_start:content_end].strip()

            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
            category = self._categorize_lesson(title)

            # –ò–∑–≤–ª–µ—á—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (–µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–¥–µ–ª –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï)
            summary = self._extract_lesson_summary(lesson_content)

            # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
            key_concepts = self._extract_key_concepts(lesson_content)

            # –†–∞–∑–±–∏—Ç—å –Ω–∞ chunks –µ—Å–ª–∏ —É—Ä–æ–∫ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
            lesson_chunks = self._chunk_text(lesson_content, chunk_size)

            for chunk_idx, chunk_text in enumerate(lesson_chunks):
                lesson = CourseLesson(
                    lesson_number=lesson_num,
                    title=title,
                    category=category,
                    content=chunk_text,
                    summary=summary if chunk_idx == 0 else None,
                    key_concepts=key_concepts if chunk_idx == 0 else [],
                    related_techniques=[],  # TODO: –∏–∑–≤–ª–µ—á—å –∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
                    source_file=str(file_path.name),
                    chunk_index=chunk_idx if len(lesson_chunks) > 1 else None,
                    total_chunks=len(lesson_chunks) if len(lesson_chunks) > 1 else None
                )

                lessons.append(lesson)

        logger.info(f"Parsed {len(lessons)} lesson chunks from {len(lesson_matches)} lessons")
        return lessons

    def _categorize_lesson(self, title: str) -> LessonCategory:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é —É—Ä–æ–∫–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        title_lower = title.lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        if any(word in title_lower for word in ['—Ç–µ—Ö–Ω–∏–∫–∞', '–º–µ—Ç–æ–¥', '—Å–ø–æ—Å–æ–±']):
            return LessonCategory.TECHNIQUE

        if any(word in title_lower for word in ['–ø—Ä–∞–∫—Ç–∏–∫–∞', '—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ', '–∑–∞–¥–∞–Ω–∏–µ']):
            return LessonCategory.PRACTICE

        if any(word in title_lower for word in ['–æ—Å–Ω–æ–≤–∞', '–≤–≤–µ–¥–µ–Ω–∏–µ', '–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—Å—è']):
            return LessonCategory.FOUNDATION

        if any(word in title_lower for word in ['—Ç–µ–æ—Ä–∏—è', '–ø–æ–Ω–∏–º–∞–Ω–∏–µ', '–∫–æ–Ω—Ü–µ–ø—Ü–∏—è']):
            return LessonCategory.THEORY

        return LessonCategory.OTHER

    def _extract_lesson_summary(self, content: str, max_length: int = 1000) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —É—Ä–æ–∫–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω: ## üìå –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï
        summary_match = re.search(
            r'## üìå –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï\s*\n+(.+?)(?=\n##|\Z)',
            content,
            re.DOTALL
        )

        if summary_match:
            summary = summary_match.group(1).strip()
            return summary[:max_length]

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        first_paragraph = content.split('\n\n')[0]
        if len(first_paragraph) > 50:
            return first_paragraph[:max_length]

        return None

    def _extract_key_concepts(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –∏–∑ —É—Ä–æ–∫–∞"""
        concepts = []

        # –ü–æ–∏—Å–∫ —Å–ø–∏—Å–∫–æ–≤ —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏ (- –∏–ª–∏ *)
        list_items = re.findall(r'^\s*[‚Ä¢\-\*]\s*(.+)$', content, re.MULTILINE)

        # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ
        for item in list_items:
            clean_item = re.sub(r'\*\*(.+?)\*\*', r'\1', item)  # –£–±—Ä–∞—Ç—å **bold**
            clean_item = clean_item.strip()

            if 10 < len(clean_item) < 200:
                concepts.append(clean_item)

        return concepts[:10]  # –¢–æ–ø 10

    def _chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """
        –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ chunks –ø–æ —Ä–∞–∑–º–µ—Ä—É (–≤ —Å–ª–æ–≤–∞—Ö)

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            chunk_size: –†–∞–∑–º–µ—Ä chunk –≤ —Å–ª–æ–≤–∞—Ö

        Returns:
            List of text chunks
        """
        words = text.split()

        if len(words) <= chunk_size:
            return [text]

        chunks = []
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i + chunk_size]
            chunks.append(' '.join(chunk_words))

        return chunks

    # ==================== CORRECTIONS PARSER ====================

    def parse_corrections(self, file_path: Path) -> List[CuratorCorrection]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ curator_corrections_ALL.json

        Returns:
            List of CuratorCorrection entities
        """
        logger.info(f"Parsing corrections from {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        corrections_data = data.get('all_corrections', [])
        corrections = []

        for idx, corr_data in enumerate(corrections_data):
            text = corr_data.get('text', '')
            author = corr_data.get('author', '–ö—É—Ä–∞—Ç–æ—Ä')
            chat = corr_data.get('chat', '')

            # –ò–∑–≤–ª–µ—á—å —Ç–∏–ø –æ—à–∏–±–∫–∏ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞
            error_type, student_text, correction, explanation = self._parse_correction_text(text)

            if not student_text or not correction:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å
                correction_entity = CuratorCorrection(
                    correction_id=f"CORR_{idx + 1}",
                    error_type="–û–±—â–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞",
                    student_text=text[:500],
                    correction=text,
                    explanation=None,
                    related_technique=None,
                    related_lesson=None,
                    curator_name=author,
                    source_file=str(file_path.name)
                )
            else:
                correction_entity = CuratorCorrection(
                    correction_id=f"CORR_{idx + 1}",
                    error_type=error_type,
                    student_text=student_text,
                    correction=correction,
                    explanation=explanation,
                    related_technique=None,  # TODO: –∏–∑–≤–ª–µ—á—å –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    related_lesson=None,     # TODO: –∏–∑–≤–ª–µ—á—å –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    curator_name=author,
                    source_file=str(file_path.name)
                )

            corrections.append(correction_entity)

        logger.info(f"Parsed {len(corrections)} corrections")
        return corrections

    def parse_questions(self, file_path: Path, sample_limit: int = 500) -> List[StudentQuestion]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ student_questions_ALL.json

        Args:
            file_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
            sample_limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ (–∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ –æ–±—ä–µ–º–∞ - 2636)

        Returns:
            List of StudentQuestion entities
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            total_questions = data.get("total_questions", 0)
            logger.info(f"Parsing student questions... Total available: {total_questions}, sampling: {sample_limit}")

            questions = []
            by_category = data.get("by_category", {})

            # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            categories = list(by_category.keys())

            # –ï—Å–ª–∏ sample_limit=None ‚Üí –∑–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï –≤–æ–ø—Ä–æ—Å—ã
            if sample_limit is None:
                per_category = None  # –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            else:
                per_category = sample_limit // len(categories) if categories else 0

            for category, category_questions in by_category.items():
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ N –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                # (–µ—Å–ª–∏ per_category=None, —Ç–æ [:None] –≤–µ—Ä–Ω—ë—Ç –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã)
                sampled = category_questions[:per_category]

                for q_data in sampled:
                    try:
                        question = StudentQuestion(
                            question_id=f"q_{len(questions)+1}",
                            question_text=q_data.get("text", "")[:2000],
                            category=category,
                            curator_answer="[–û—Ç–≤–µ—Ç –∫—É—Ä–∞—Ç–æ—Ä–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏]"  # Placeholder
                        )
                        questions.append(question)
                    except Exception as e:
                        logger.warning(f"Failed to parse question: {e}")
                        continue

            logger.info(f"‚úÖ Parsed {len(questions)} student questions from {len(categories)} categories")
            return questions

        except Exception as e:
            logger.error(f"Failed to parse student questions from {file_path}: {e}")
            return []

    def parse_brainwrites(self, file_path: Path, sample_limit: int = 200) -> List[BrainwriteExample]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ student_brainwrites_SAMPLE.json

        Args:
            file_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
            sample_limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤

        Returns:
            List of BrainwriteExample entities
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            total_brainwrites = data.get("total_brainwrites", 0)
            logger.info(f"Parsing brainwrites... Total available: {total_brainwrites}, sampling: {sample_limit}")

            brainwrites = []
            categories_data = data.get("categories", {})

            # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            categories = list(categories_data.keys())
            per_category = sample_limit // len(categories) if categories else 0

            for category_name, category_data in categories_data.items():
                examples = category_data.get("examples", [])
                sampled = examples[:per_category]

                for ex_data in sampled:
                    try:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –¥–ª–∏–Ω–µ (–¥–ª–∏–Ω–Ω—ã–µ = –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ)
                        text = ex_data.get("text", "")
                        length = len(text)

                        if length > 3000:
                            quality = "excellent"
                        elif length > 1500:
                            quality = "good"
                        else:
                            quality = "average"

                        brainwrite = BrainwriteExample(
                            brainwrite_id=f"bw_{len(brainwrites)+1}",
                            text=text[:3000],  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É
                            student_name=ex_data.get("author", "Unknown"),
                            technique_used=category_name,
                            quality_rating=quality
                        )
                        brainwrites.append(brainwrite)
                    except Exception as e:
                        logger.warning(f"Failed to parse brainwrite: {e}")
                        continue

            logger.info(f"‚úÖ Parsed {len(brainwrites)} brainwrite examples from {len(categories)} categories")
            return brainwrites

        except Exception as e:
            logger.error(f"Failed to parse brainwrites from {file_path}: {e}")
            return []

    def _parse_correction_text(self, text: str) -> Tuple[str, str, str, Optional[str]]:
        """
        –ò–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏

        Returns:
            (error_type, student_text, correction, explanation)
        """
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫
        patterns = {
            "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞": r'—Å–æ–±–ª—é–¥–∞–π—Ç–µ.*?—Å—Ç—Ä—É–∫—Ç—É—Ä—É',
            "–≤–æ–ø—Ä–æ—Å 5": r'–¥–æ–ø–∏—à–∏—Ç–µ.*?–æ—Ç–≤–µ—Ç –Ω–∞ 5 –≤–æ–ø—Ä–æ—Å',
            "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": r'—Ä–µ–∫–æ–º–µ–Ω–¥—É—é.*?–ø—Ä–æ–ø–∏—Å–∞—Ç—å',
            "–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ": r'–º–æ–∂–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç—å',
        }

        error_type = "–û–±—â–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞"
        for err_type, pattern in patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                error_type = err_type.capitalize()
                break

        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞: –≤–µ—Å—å —Ç–µ–∫—Å—Ç = –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞
        return error_type, "", text, None

    # ==================== GLOSSARY PARSER ====================

    def parse_glossary(self, file_path: Path) -> List[GlossaryEntry]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Å–µ–∫—Ü–∏–π –ö–õ–Æ–ß–ï–í–´–ï –¢–ï–†–ú–ò–ù–´ –ò –ü–û–ù–Ø–¢–ò–Ø

        Args:
            file_path: –ü—É—Ç—å –∫ KNOWLEDGE_BASE_FULL.md

        Returns:
            List of GlossaryEntry entities
        """
        logger.info(f"Parsing glossary from {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        glossary_entries = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Å–µ–∫—Ü–∏–π —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        section_pattern = r'(?:##?\s*)?üìö\s*(?:\*\*)?–ö–õ–Æ–ß–ï–í–´–ï –¢–ï–†–ú–ò–ù–´–ò –ü–û–ù–Ø–¢–ò–Ø(?:\*\*)?:?\s*\n((?:- \*\*[^*]+\*\*:[^\n]+\n?)+)'

        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –≤–∞—Ä–∏–∞—Ü–∏–π
        alt_section_pattern = r'üìö\s*(?:\*\*)?–ö–õ–Æ–ß–ï–í–´–ï –¢–ï–†–ú–ò–ù–´(?:\*\*)?:?\s*\n((?:- \*\*[^*]+\*\*:[^\n]+\n?)+)'

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        term_pattern = r'- \*\*([^*]+)\*\*:\s*(.+)'

        # –ù–∞–π—Ç–∏ –≤—Å–µ —Å–µ–∫—Ü–∏–∏ —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        sections = list(re.finditer(section_pattern, content, re.IGNORECASE))
        sections.extend(list(re.finditer(alt_section_pattern, content, re.IGNORECASE)))

        # –ï—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—â–µ–º —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞–ø—Ä—è–º—É—é
        if not sections:
            logger.info("Using direct term search (no sections found)")
            # –ù–∞–π—Ç–∏ –≤—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ - **–¢–µ—Ä–º–∏–Ω**: –æ–ø–∏—Å–∞–Ω–∏–µ
            all_terms = re.findall(term_pattern, content)

            for idx, (term, definition) in enumerate(all_terms):
                term = term.strip()
                definition = definition.strip()

                if len(definition) < 10:
                    continue

                entry = GlossaryEntry(
                    term_id=f"glossary_{idx}",
                    term=term,
                    definition=definition,
                    lesson_number=None,
                    keywords=self._extract_keywords(f"{term} {definition}"),
                    source_file=str(file_path.name)
                )
                glossary_entries.append(entry)
        else:
            # –ü–∞—Ä—Å–∏—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
            term_idx = 0
            for section_match in sections:
                section_content = section_match.group(1) if section_match.lastindex else section_match.group(0)

                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–æ–º–µ—Ä —É—Ä–æ–∫–∞ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ñ–∞–π–ª–µ
                lesson_number = self._find_lesson_number(content, section_match.start())

                # –ò–∑–≤–ª–µ—á—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ —Å–µ–∫—Ü–∏–∏
                terms = re.findall(term_pattern, section_content)

                for term, definition in terms:
                    term = term.strip()
                    definition = definition.strip()

                    if len(definition) < 10:
                        continue

                    entry = GlossaryEntry(
                        term_id=f"glossary_{term_idx}",
                        term=term,
                        definition=definition,
                        lesson_number=lesson_number,
                        keywords=self._extract_keywords(f"{term} {definition}"),
                        source_file=str(file_path.name)
                    )
                    glossary_entries.append(entry)
                    term_idx += 1

        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ—Ä–º–∏–Ω—É
        seen_terms = set()
        unique_entries = []
        for entry in glossary_entries:
            if entry.term.lower() not in seen_terms:
                seen_terms.add(entry.term.lower())
                unique_entries.append(entry)

        logger.info(f"Parsed {len(unique_entries)} unique glossary terms")
        return unique_entries

    def _find_lesson_number(self, content: str, position: int) -> Optional[int]:
        """–ù–∞–π—Ç–∏ –Ω–æ–º–µ—Ä —É—Ä–æ–∫–∞ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ñ–∞–π–ª–µ"""
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É—Ä–æ–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–∑–∏—Ü–∏–µ–π
        lesson_pattern = r'# –£–†–û–ö (\d+):'
        matches = list(re.finditer(lesson_pattern, content[:position]))
        if matches:
            return int(matches[-1].group(1))
        return None

    # ==================== MAIN PARSING ====================

    def parse_all(self) -> Dict[str, List[Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

        Returns:
            Dict —Å parsed entities –ø–æ —Ç–∏–ø–∞–º
        """
        logger.info(f"Starting full knowledge base parsing from {self.kb_dir}")

        results = {
            "faq": [],
            "lessons": [],
            "corrections": [],
            "questions": [],
            "brainwrites": [],
            "glossary": []
        }

        # 1. Parse FAQ
        faq_file = self.kb_dir / "FAQ_EXTENDED.md"
        if faq_file.exists():
            results["faq"] = self.parse_faq(faq_file)
        else:
            logger.warning(f"FAQ file not found: {faq_file}")

        # 2. Parse Lessons
        lessons_file = self.kb_dir / "KNOWLEDGE_BASE_FULL.md"
        if lessons_file.exists():
            results["lessons"] = self.parse_lessons(lessons_file, chunk_size=800)
        else:
            logger.warning(f"Lessons file not found: {lessons_file}")

        # 3. Parse Corrections
        corrections_file = self.kb_dir / "curator_corrections_ALL.json"
        if corrections_file.exists():
            results["corrections"] = self.parse_corrections(corrections_file)
        else:
            logger.warning(f"Corrections file not found: {corrections_file}")

        # 4. Parse Student Questions
        questions_file = self.kb_dir / "student_questions_ALL.json"
        if questions_file.exists():
            results["questions"] = self.parse_questions(questions_file, sample_limit=500)
        else:
            logger.warning(f"Questions file not found: {questions_file}")

        # 5. Parse Brainwrite Examples
        brainwrites_file = self.kb_dir / "student_brainwrites_SAMPLE.json"
        if brainwrites_file.exists():
            results["brainwrites"] = self.parse_brainwrites(brainwrites_file, sample_limit=200)
        else:
            logger.warning(f"Brainwrites file not found: {brainwrites_file}")

        # 6. Parse Glossary Terms
        glossary_file = self.kb_dir / "KNOWLEDGE_BASE_FULL.md"
        if glossary_file.exists():
            results["glossary"] = self.parse_glossary(glossary_file)
        else:
            logger.warning(f"Glossary source file not found: {glossary_file}")

        # Summary
        total_entities = sum(len(v) for v in results.values())
        logger.info(f"\n{'=' * 60}")
        logger.info(f"PARSING COMPLETE: {total_entities} total entities")
        logger.info(f"  FAQ entries: {len(results['faq'])}")
        logger.info(f"  Lesson chunks: {len(results['lessons'])}")
        logger.info(f"  Corrections: {len(results['corrections'])}")
        logger.info(f"  Questions: {len(results['questions'])}")
        logger.info(f"  Brainwrites: {len(results['brainwrites'])}")
        logger.info(f"  Glossary terms: {len(results['glossary'])}")
        logger.info(f"{'=' * 60}\n")

        return results

    def save_parsed_data(self, results: Dict[str, List[Any]], output_dir: Path):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å parsed entities –≤ JSON —Ñ–∞–π–ª—ã

        Args:
            results: Dict —Å parsed entities
            output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        output_dir.mkdir(parents=True, exist_ok=True)

        for entity_type, entities in results.items():
            if not entities:
                continue

            output_file = output_dir / f"parsed_{entity_type}.json"

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Pydantic models –≤ dict
            entities_data = [e.model_dump() for e in entities]

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(entities_data, f, ensure_ascii=False, indent=2, default=str)

            logger.info(f"Saved {len(entities)} {entity_type} to {output_file}")


def main():
    """Main function"""
    kb_dir = Path(__file__).parent.parent / "KNOWLEDGE_BASE"

    if not kb_dir.exists():
        logger.error(f"Knowledge base directory not found: {kb_dir}")
        return

    parser = KnowledgeBaseParser(kb_dir)

    # Parse all
    results = parser.parse_all()

    # Save parsed data
    output_dir = Path(__file__).parent.parent / "data" / "parsed_kb"
    parser.save_parsed_data(results, output_dir)

    logger.info("‚úÖ Parsing complete! Parsed data saved to data/parsed_kb/")


if __name__ == "__main__":
    main()
