"""
Knowledge Search Service

–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫—É—Ä—Å–∞ "–í—Å–µ–ø—Ä–æ—â–∞—é—â–∞—è" –∏—Å–ø–æ–ª—å–∑—É—è:
- Graphiti semantic search (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)
- Neo4j full-text search (keyword matching)
- Graph traversal (–ø–æ–∏—Å–∫ –ø–æ relationships)
- Fallback –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º –µ—Å–ª–∏ Graphiti –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω

Architecture:
    User Query ‚Üí Query Routing ‚Üí Hybrid Search ‚Üí Ranked Results ‚Üí LLM Context
"""

import os
import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from enum import Enum

from bot.services.falkordb_service import get_falkordb_service  # FalkorDB (496x faster than Neo4j!)
from bot.services.qdrant_service import get_qdrant_service
from bot.config import GRAPHITI_ENABLED, USE_QDRANT

logger = logging.getLogger(__name__)


class SearchStrategy(str, Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞"""
    SEMANTIC = "semantic"      # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (embeddings)
    FULLTEXT = "fulltext"      # Keyword matching (BM25)
    GRAPH = "graph"            # Graph traversal (relationships)
    HYBRID = "hybrid"          # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
    FALLBACK = "fallback"      # –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã (–∫–æ–≥–¥–∞ Graphiti –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)


class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""

    def __init__(
        self,
        content: str,
        source: str,
        relevance_score: float,
        metadata: Optional[Dict[str, Any]] = None,
        search_type: str = "unknown"
    ):
        self.content = content
        self.source = source
        self.relevance_score = relevance_score
        self.metadata = metadata or {}
        self.search_type = search_type

    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ dict"""
        return {
            "content": self.content,
            "source": self.source,
            "relevance_score": self.relevance_score,
            "metadata": self.metadata,
            "search_type": self.search_type
        }

    def __repr__(self):
        return f"SearchResult(source={self.source}, score={self.relevance_score:.2f}, type={self.search_type})"


class KnowledgeSearchService:
    """
    –°–µ—Ä–≤–∏—Å –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π

    Features:
    - Automatic query routing (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏)
    - Hybrid search (semantic + fulltext + graph)
    - Result ranking and deduplication
    - Fallback to local files
    - Context formatting for LLM
    """

    def __init__(self):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–µ–∏—Ö —Å–∏—Å—Ç–µ–º
        self.falkordb_service = get_falkordb_service()  # FalkorDB (496x faster!)
        self.qdrant_service = get_qdrant_service()

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–∫—É—é —Å–∏—Å—Ç–µ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        self.use_qdrant = USE_QDRANT
        self.graphiti_enabled = GRAPHITI_ENABLED and self.falkordb_service.enabled
        self.qdrant_enabled = USE_QDRANT and self.qdrant_service.enabled

        # Paths –¥–ª—è fallback
        self.kb_dir = Path(__file__).parent.parent.parent / "KNOWLEDGE_BASE"

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
        if self.use_qdrant and self.qdrant_enabled:
            logger.info("üîµ KnowledgeSearchService initialized (Using: QDRANT)")
        elif self.graphiti_enabled:
            logger.info("üü¢ KnowledgeSearchService initialized (Using: GRAPHITI)")
        else:
            logger.info("‚ö™ KnowledgeSearchService initialized (Using: FALLBACK - local files)")

    async def search(
        self,
        query: str,
        strategy: SearchStrategy = SearchStrategy.HYBRID,
        limit: int = 5,
        min_relevance: float = 0.6
    ) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∏—Å–∫–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_relevance: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ relevance (0-1)

        Returns:
            List of SearchResult sorted by relevance
        """
        logger.info(f"Search query: '{query}' (strategy: {strategy}, limit: {limit})")

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–∫—É—é —Å–∏—Å—Ç–µ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        if self.use_qdrant and self.qdrant_enabled:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Qdrant
            logger.info("üîµ Using Qdrant for search")
            if strategy == SearchStrategy.GRAPH:
                # Qdrant –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç graph traversal - fallback –Ω–∞ semantic
                logger.warning("Qdrant doesn't support graph traversal, using semantic search instead")
                strategy = SearchStrategy.SEMANTIC
        elif self.graphiti_enabled:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Graphiti
            logger.info("üü¢ Using Graphiti for search")
        else:
            # Fallback –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º
            logger.warning("Both Qdrant and Graphiti disabled, using fallback to local files")
            return await self._search_fallback(query, limit)

        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if strategy == SearchStrategy.SEMANTIC:
            results = await self._search_semantic(query, limit, min_relevance)
        elif strategy == SearchStrategy.FULLTEXT:
            results = await self._search_fulltext(query, limit, min_relevance)
        elif strategy == SearchStrategy.GRAPH:
            results = await self._search_graph(query, limit, min_relevance)
        elif strategy == SearchStrategy.HYBRID:
            results = await self._search_hybrid(query, limit, min_relevance)
        else:
            logger.error(f"Unknown strategy: {strategy}")
            results = []

        logger.info(f"Search returned {len(results)} results")
        return results

    async def _search_semantic(
        self,
        query: str,
        limit: int,
        min_relevance: float
    ) -> List[SearchResult]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Qdrant –∏–ª–∏ Graphiti

        –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è entity types –¥–ª—è –º–æ–∑–≥–æ—Ä–∏—Ç–º–æ–≤:
        1. Lessons (boost 1.5x) - –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –º–æ–∑–≥–æ—Ä–∏—Ç–º–æ–≤
        2. Corrections (boost 1.2x) - —Å—Ç–∏–ª—å –∏ –ø—Ä–∏–º–µ—Ä—ã —Ñ—Ä–∞–∑
        3. FAQ - –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
        4. Brainwrites –ò–°–ö–õ–Æ–ß–ï–ù–´ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏)
        """
        try:
            # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É Qdrant –∏ Graphiti
            if self.use_qdrant and self.qdrant_enabled:
                # –ú–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –ø–æ–∏—Å–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
                all_results = []

                # –≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ –≤ –£–†–û–ö–ê–• (highest priority)
                logger.info(f"üîç –≠—Ç–∞–ø 1: –ü–æ–∏—Å–∫ –≤ —É—Ä–æ–∫–∞—Ö (lessons)...")
                lesson_results = await self.qdrant_service.search_semantic(
                    query=query,
                    limit=limit,
                    score_threshold=min_relevance,
                    entity_type="lesson"  # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É!
                )

                for r in lesson_results:
                    # Score boost –¥–ª—è —É—Ä–æ–∫–æ–≤
                    boosted_score = r.get("score", 0.0) * 1.5
                    metadata = {**r.get("metadata", {}), "entity_type": r.get("entity_type", "lesson")}
                    result = SearchResult(
                        content=r.get("content", ""),
                        source=f"qdrant_{r.get('entity_type', 'lesson')}",
                        relevance_score=boosted_score,
                        metadata=metadata,
                        search_type="semantic_qdrant_prioritized"
                    )
                    all_results.append(result)

                logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(lesson_results)} lessons (boosted score 1.5x)")

                # –≠–¢–ê–ü 2: –ü–æ–∏—Å–∫ –≤ –ö–û–†–†–ï–ö–¢–ò–†–û–í–ö–ê–• –ö–£–†–ê–¢–û–†–ê (medium priority)
                logger.info(f"üîç –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ –≤ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞—Ö –∫—É—Ä–∞—Ç–æ—Ä–∞ (corrections)...")
                correction_results = await self.qdrant_service.search_semantic(
                    query=query,
                    limit=limit,
                    score_threshold=min_relevance,
                    entity_type="correction"
                )

                for r in correction_results:
                    # Score boost –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫
                    boosted_score = r.get("score", 0.0) * 1.2
                    metadata = {**r.get("metadata", {}), "entity_type": r.get("entity_type", "correction")}
                    result = SearchResult(
                        content=r.get("content", ""),
                        source=f"qdrant_{r.get('entity_type', 'correction')}",
                        relevance_score=boosted_score,
                        metadata=metadata,
                        search_type="semantic_qdrant_prioritized"
                    )
                    all_results.append(result)

                logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(correction_results)} corrections (boosted score 1.2x)")

                # –≠–¢–ê–ü 3: –ü–æ–∏—Å–∫ –≤ FAQ (–µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É—Ä–æ–∫–æ–≤ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫)
                if len(all_results) < limit:
                    logger.info(f"üîç –≠—Ç–∞–ø 3: –ü–æ–∏—Å–∫ –≤ FAQ...")
                    faq_results = await self.qdrant_service.search_semantic(
                        query=query,
                        limit=limit - len(all_results),
                        score_threshold=min_relevance,
                        entity_type="faq"
                    )

                    for r in faq_results:
                        metadata = {**r.get("metadata", {}), "entity_type": r.get("entity_type", "faq")}
                        result = SearchResult(
                            content=r.get("content", ""),
                            source=f"qdrant_{r.get('entity_type', 'faq')}",
                            relevance_score=r.get("score", 0.0),
                            metadata=metadata,
                            search_type="semantic_qdrant_prioritized"
                        )
                        all_results.append(result)

                    logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(faq_results)} FAQ")

                # NOTE: Brainwrites –ò–°–ö–õ–Æ–ß–ï–ù–´ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞!
                # –û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —è–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω—ã –æ—Ç–¥–µ–ª—å–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
                logger.info(f"‚ö†Ô∏è Brainwrites –∏—Å–∫–ª—é—á–µ–Ω—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏)")

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ boosted relevance score
                all_results.sort(key=lambda x: x.relevance_score, reverse=True)

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º top N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                final_results = all_results[:limit]
                logger.info(f"üìä –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (lessons: {len(lesson_results)}, corrections: {len(correction_results)})")

                return final_results

            else:
                # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ FalkorDB (Graphiti backend)
                graphiti_results = await self.falkordb_service.search_semantic(
                    query=query,
                    limit=limit,
                    min_relevance=min_relevance  # Note: –ø–∞—Ä–∞–º–µ—Ç—Ä –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω
                )

                results = []
                for r in graphiti_results:
                    result = SearchResult(
                        content=r.get("content", ""),
                        source=r.get("source", "knowledge_base"),
                        relevance_score=r.get("similarity", 0.0),
                        metadata=r.get("metadata", {}),
                        search_type="semantic_graphiti"
                    )
                    results.append(result)

                return results

        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            logger.exception("Full traceback:")
            return []

    async def _search_fulltext(
        self,
        query: str,
        limit: int,
        min_relevance: float
    ) -> List[SearchResult]:
        """Full-text search —á–µ—Ä–µ–∑ Neo4j"""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è fulltext index –≤ Neo4j
        logger.warning("Fulltext search not implemented yet")
        return []

    async def _search_graph(
        self,
        query: str,
        limit: int,
        min_relevance: float
    ) -> List[SearchResult]:
        """Graph traversal search"""
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ relationships
        logger.warning("Graph search not implemented yet")
        return []

    async def _search_hybrid(
        self,
        query: str,
        limit: int,
        min_relevance: float
    ) -> List[SearchResult]:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è semantic + fulltext + graph)

        –î–ª—è Qdrant: multi-stage search —Å entity_type –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
        - –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è: lessons (1.5x) > corrections (1.2x) > FAQ (1.0x)
        - Brainwrites –∏ questions –ò–°–ö–õ–Æ–ß–ï–ù–´ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

        –î–ª—è Graphiti: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è semantic + fulltext + graph

        –í–µ—Å–∞:
        - Semantic: 1.0 (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        - Fulltext: 0.8
        - Graph: 0.6
        """
        # –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É Qdrant –∏ Graphiti
        if self.use_qdrant and self.qdrant_enabled:
            # Qdrant multi-stage search —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
            try:
                all_results = []

                # –≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ –≤ –£–†–û–ö–ê–• (highest priority)
                logger.info(f"üîç Hybrid: –≠—Ç–∞–ø 1 - –ü–æ–∏—Å–∫ –≤ —É—Ä–æ–∫–∞—Ö (lessons)...")
                lesson_results = await self.qdrant_service.search_semantic(
                    query=query,
                    limit=limit,
                    score_threshold=min_relevance,
                    entity_type="lesson"
                )

                for r in lesson_results:
                    boosted_score = r.get("score", 0.0) * 1.5
                    metadata = {**r.get("metadata", {}), "entity_type": r.get("entity_type", "lesson")}
                    result = SearchResult(
                        content=r.get("content", ""),
                        source=f"qdrant_{r.get('entity_type', 'lesson')}",
                        relevance_score=boosted_score,
                        metadata=metadata,
                        search_type="hybrid_qdrant_prioritized"
                    )
                    all_results.append(result)

                logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(lesson_results)} lessons (boosted 1.5x)")

                # –≠–¢–ê–ü 2: –ü–æ–∏—Å–∫ –≤ –ö–û–†–†–ï–ö–¢–ò–†–û–í–ö–ê–• –ö–£–†–ê–¢–û–†–ê (medium priority)
                logger.info(f"üîç Hybrid: –≠—Ç–∞–ø 2 - –ü–æ–∏—Å–∫ –≤ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞—Ö (corrections)...")
                correction_results = await self.qdrant_service.search_semantic(
                    query=query,
                    limit=limit,
                    score_threshold=min_relevance,
                    entity_type="correction"
                )

                for r in correction_results:
                    boosted_score = r.get("score", 0.0) * 1.2
                    metadata = {**r.get("metadata", {}), "entity_type": r.get("entity_type", "correction")}
                    result = SearchResult(
                        content=r.get("content", ""),
                        source=f"qdrant_{r.get('entity_type', 'correction')}",
                        relevance_score=boosted_score,
                        metadata=metadata,
                        search_type="hybrid_qdrant_prioritized"
                    )
                    all_results.append(result)

                logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(correction_results)} corrections (boosted 1.2x)")

                # –≠–¢–ê–ü 3: –ü–æ–∏—Å–∫ –≤ FAQ (–µ—Å–ª–∏ –µ—â—ë –Ω—É–∂–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
                if len(all_results) < limit:
                    logger.info(f"üîç Hybrid: –≠—Ç–∞–ø 3 - –ü–æ–∏—Å–∫ –≤ FAQ...")
                    faq_results = await self.qdrant_service.search_semantic(
                        query=query,
                        limit=limit - len(all_results),
                        score_threshold=min_relevance,
                        entity_type="faq"
                    )

                    for r in faq_results:
                        metadata = {**r.get("metadata", {}), "entity_type": r.get("entity_type", "faq")}
                        result = SearchResult(
                            content=r.get("content", ""),
                            source=f"qdrant_{r.get('entity_type', 'faq')}",
                            relevance_score=r.get("score", 0.0),
                            metadata=metadata,
                            search_type="hybrid_qdrant_prioritized"
                        )
                        all_results.append(result)

                    logger.info(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(faq_results)} FAQ")

                # NOTE: Brainwrites –∏ questions –ò–°–ö–õ–Æ–ß–ï–ù–´ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞!
                logger.info(f"‚ö†Ô∏è Hybrid: Brainwrites –∏ questions –∏—Å–∫–ª—é—á–µ–Ω—ã (–º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—à–∏–±–∫–∏)")

                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ relevance score (—É–∂–µ —Å —É—á—ë—Ç–æ–º boost)
                all_results.sort(key=lambda x: x.relevance_score, reverse=True)

                return all_results[:limit]

            except Exception as e:
                logger.error(f"Qdrant hybrid search failed: {e}")
                return []

        else:
            # Graphiti hybrid search (semantic + fulltext + graph)
            all_results = []

            # 1. Semantic search
            semantic_results = await self._search_semantic(query, limit * 2, min_relevance)
            for r in semantic_results:
                r.relevance_score *= 1.0  # –í–µ—Å semantic
            all_results.extend(semantic_results)

            # 2. Fulltext search (–∫–æ–≥–¥–∞ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –¥–ª—è Graphiti)
            # fulltext_results = await self._search_fulltext(query, limit, min_relevance)
            # for r in fulltext_results:
            #     r.relevance_score *= 0.8
            # all_results.extend(fulltext_results)

            # 3. Graph search (–∫–æ–≥–¥–∞ —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –¥–ª—è Graphiti)
            # graph_results = await self._search_graph(query, limit, min_relevance)
            # for r in graph_results:
            #     r.relevance_score *= 0.6
            # all_results.extend(graph_results)

            # Deduplicate (–ø–æ content hash)
            unique_results = self._deduplicate_results(all_results)

            # Sort by relevance
            unique_results.sort(key=lambda x: x.relevance_score, reverse=True)

            return unique_results[:limit]

    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """–£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        seen = set()
        unique = []

        for result in results:
            content_hash = hash(result.content[:200])  # –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(result)

        return unique

    async def _search_fallback(self, query: str, limit: int) -> List[SearchResult]:
        """
        Fallback –ø–æ–∏—Å–∫ –ø–æ –ª–æ–∫–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º (–∫–æ–≥–¥–∞ Graphiti –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)

        –ü—Ä–æ—Å—Ç–æ–π keyword matching –≤ markdown —Ñ–∞–π–ª–∞—Ö
        """
        logger.info("Using fallback search (local files)")
        results = []

        # –ü–æ–∏—Å–∫ –≤ FAQ
        faq_file = self.kb_dir / "FAQ_EXTENDED.md"
        if faq_file.exists():
            faq_results = self._search_in_file(faq_file, query, "FAQ")
            results.extend(faq_results[:limit // 2])

        # –ü–æ–∏—Å–∫ –≤ —É—Ä–æ–∫–∞—Ö
        lessons_file = self.kb_dir / "KNOWLEDGE_BASE_FULL.md"
        if lessons_file.exists():
            lesson_results = self._search_in_file(lessons_file, query, "Lessons")
            results.extend(lesson_results[:limit // 2])

        # Sort by relevance
        results.sort(key=lambda x: x.relevance_score, reverse=True)

        return results[:limit]

    def _search_in_file(
        self,
        file_path: Path,
        query: str,
        source: str
    ) -> List[SearchResult]:
        """
        –ü—Ä–æ—Å—Ç–æ–π keyword –ø–æ–∏—Å–∫ –≤ —Ñ–∞–π–ª–µ

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ (–¥–ª—è metadata)

        Returns:
            List of SearchResult
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # –†–∞–∑–±–∏—Ç—å –Ω–∞ —Å–µ–∫—Ü–∏–∏ (–ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º ##)
            sections = re.split(r'\n## ', content)

            results = []
            query_lower = query.lower()
            query_words = set(query_lower.split())

            for section in sections:
                section_lower = section.lower()

                # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                matches = sum(1 for word in query_words if word in section_lower)

                if matches > 0:
                    # Relevance = matches / total query words
                    relevance = matches / len(query_words) if query_words else 0

                    # –ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏
                    section_title = section.split('\n')[0] if section else "Unknown"

                    # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä content
                    section_content = section[:1500] + "..." if len(section) > 1500 else section

                    result = SearchResult(
                        content=section_content,
                        source=f"{source}: {section_title}",
                        relevance_score=relevance,
                        metadata={"file": str(file_path.name)},
                        search_type="fallback"
                    )
                    results.append(result)

            # Sort by relevance
            results.sort(key=lambda x: x.relevance_score, reverse=True)

            return results

        except Exception as e:
            logger.error(f"Failed to search in {file_path}: {e}")
            return []

    def route_query(self, query: str) -> SearchStrategy:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞

        Rules:
        - "—É—Ä–æ–∫ N" –∏–ª–∏ "lesson N" ‚Üí FULLTEXT (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        - "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫–∞–∫ –ø–æ–Ω—è—Ç—å" ‚Üí SEMANTIC (–∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫)
        - "–ø–æ—Ö–æ–∂–∏–µ –ø—Ä–∏–º–µ—Ä—ã", "—Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏" ‚Üí GRAPH (relationships)
        - Default ‚Üí HYBRID (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤)

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è SearchStrategy
        """
        query_lower = query.lower()

        # –¢–æ—á–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ —É—Ä–æ–∫–æ–≤
        if re.search(r'—É—Ä–æ–∫\s+\d+|lesson\s+\d+', query_lower):
            logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è FULLTEXT (–Ω–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω '—É—Ä–æ–∫ N' –≤ –∑–∞–ø—Ä–æ—Å–µ)")
            return SearchStrategy.FULLTEXT

        # –ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        if any(word in query_lower for word in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫–∞–∫ –ø–æ–Ω—è—Ç—å", "–æ–±—ä—è—Å–Ω–∏", "–≤ —á–µ–º —Å–º—ã—Å–ª"]):
            logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è SEMANTIC (–∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å)")
            return SearchStrategy.SEMANTIC

        # –ü–æ–∏—Å–∫ —Å–≤—è–∑–µ–π
        if any(word in query_lower for word in ["–ø–æ—Ö–æ–∂–∏–µ", "—Å–≤—è–∑–∞–Ω–Ω—ã–µ", "—Å–º–µ–∂–Ω—ã–µ", "related"]):
            logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è GRAPH (–ø–æ–∏—Å–∫ —Å–≤—è–∑–µ–π)")
            return SearchStrategy.GRAPH

        # Default: hybrid
        logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è HYBRID (default - –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫)")
        return SearchStrategy.HYBRID

    def format_context_for_llm(
        self,
        results: List[SearchResult],
        max_length: int = 3000
    ) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM

        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)

        Returns:
            Formatted context string
        """
        if not results:
            return "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."

        context_parts = ["–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n"]
        current_length = len(context_parts[0])

        for i, result in enumerate(results, 1):
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            header = f"\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {result.source} (relevance: {result.relevance_score:.2f})\n"

            # –ö–æ–Ω—Ç–µ–Ω—Ç (–æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            content = result.content

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–∏–º–∏—Ç
            if current_length + len(header) + len(content) > max_length:
                # –û–±—Ä–µ–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                remaining = max_length - current_length - len(header)
                if remaining > 100:
                    content = content[:remaining] + "..."
                    context_parts.append(header + content)
                break
            else:
                context_parts.append(header + content)
                current_length += len(header) + len(content)

        return "\n".join(context_parts)


# Singleton instance
_knowledge_search_service = None

def get_knowledge_search_service() -> KnowledgeSearchService:
    """
    –ü–æ–ª—É—á–∏—Ç—å singleton instance Knowledge Search Service

    Returns:
        KnowledgeSearchService instance
    """
    global _knowledge_search_service
    if _knowledge_search_service is None:
        _knowledge_search_service = KnowledgeSearchService()
    return _knowledge_search_service
